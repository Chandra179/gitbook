# Big Integer

We often use decimal for money, which allows for a value like `$10.5000001` . However, in a transactional context, money is actually discrete; it consists of individual, indivisible units. By using `BigInt` to store `$10.50` as exactly `1050` cents, we physically prevent the storage of "half a cent." This approach enforces a hard constraint on data integrity, ensuring that any rounding logic is handled explicitly before the data ever reaches the database, rather than relying on ambiguous floating-point interpretations later.

Beyond integrity, there is a distinct, performance cost to using Decimals. A standard `BIGINT` occupies exactly 8 bytes and fits perfectly into a 64-bit CPU register, making mathematical operations like addition and subtraction blazing fastâ€”often requiring just a single CPU instruction.&#x20;

In contrast, database `DECIMAL` types can consume anywhere from 5 to 17 bytes. Worse, the CPU cannot process them natively; it must rely on software algorithms to simulate the math. While this overhead might seem negligible in isolation, it compounds aggressively when processing millions of ledger rows, creating a performance drag that offers no functional benefit.

The most dangerous issues arise when it attempts to pass the data between systems. `BigDecimal` typically behaves well inside a single Java or C# class, but when passed the data to database, a frontend, or another microservice it will become inconsistent.

Consider a backend sending a precise decimal like `{"amount": 100.50}`. A JavaScript frontend or Node.js service receiving this will treat it as an IEEE 754 Float. A simple operation like adding ten cents (`response.amount + 0.10`) can result in `100.60000000000001`. You have corrupted transaction data simply by moving it between services. Conversely, usage of Integers eliminates this ambiguity entirely. If you transmit `{"amount": 10050}`, every language in the world Java, Go, Python, Rust, JavaScript understands exactly what that number is. `10050 + 10` is always `10060`. There is no interpretation required.

Finally, using Integers forces developers to write safer code by preventing the "Non-Terminating" crash. If you attempt to split a **$100** bill among three users using `BigDecimal`, the system attempts to calculate `33.3333333...` to infinity, often throwing an `ArithmeticException` and crashing the application. Integer division (`10000 / 3`) naturally results in `3333` with a remainder of `1`. This constraint forces the engineer to acknowledge the "extra penny" and write logic to handle it, rather than letting the server crash or the money vanish into a floating-point void.

This architectural approach is not theoretical; it is standard of the financial industry. The Ruby on Rails team famously had to force `BigDecimal` to serialize as Strings because transmitting them as numbers was corrupting data in JavaScript. Similarly, Stripe strictly uses Integers for all transactions. As their API documentation states, they require "a positive integer representing how much to charge in the smallest currency unit (e.g., 100 cents to charge $1.00)."
