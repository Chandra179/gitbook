# Math

* **Algebra 1 & 2**: Solving for $$x$$, exponents, logarithms ($$log$$), and function notation $$f(x)$$.
* **Trigonometry**: The unit circle, sine, and cosine. (Essential for fields like Computer Graphics or Signal Processing).
* **Pre-Calculus**: Combining algebra and trig to understand limits and complex functions.
* **Calculus 1 (Differential)**: Learning about the Derivative. In AI, this is how we calculate "error" change.
* **Calculus 2 (Integral)**: Learning about the Integral (area under a curve). Essential for continuous probability.
* **Calculus 3 (Multivariable)**: Taking derivatives of functions with many variables. This is the math that actually powers Gradient Descent in Machine Learning.
* **Linear Algebra**: The study of vectors and matrices. This is the "language" of data. Every pixel in an image or word in a Large Language Model is stored as a vector.
* **Discrete Mathematics**: Unlike Calculus (which is continuous), this deals with "distinct" objects. It covers logic, set theory, graph theory (networks), and combinatorics (counting). This is the foundation for Algorithms.
* **Probability & Statistics**: You use Calculus and Set Theory here to model uncertainty. In a Master's program, you move from "What is the average?" to "How likely is this AI model to be correct?"
* **Mathematical Proofs**: Many Master's programs expect you to move beyond _calculating_ an answer to _proving_ that an algorithm will always work.







